{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing _logistic regression_ using Python and `numpy` <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Logistic-function\" data-toc-modified-id=\"Logistic-function-1\">Logistic function</a></span></li><li><span><a href=\"#Likelihood-function\" data-toc-modified-id=\"Likelihood-function-2\">Likelihood function</a></span></li><li><span><a href=\"#Fitting:-training-predictors-+-training-response---&gt;-model-paramenters\" data-toc-modified-id=\"Fitting:-training-predictors-+-training-response--->-model-paramenters-3\">Fitting: training predictors + training response --&gt; model paramenters</a></span></li><li><span><a href=\"#Predicting:-test-predictors----[model(parameters)]---&gt;-test-response\" data-toc-modified-id=\"Predicting:-test-predictors----[model(parameters)]--->-test-response-4\">Predicting: test predictors -- [model(parameters)] --&gt; test response</a></span></li><li><span><a href=\"#Test-implementation\" data-toc-modified-id=\"Test-implementation-5\">Test implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-sample-dataset\" data-toc-modified-id=\"Load-sample-dataset-5.1\">Load sample dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fit\" data-toc-modified-id=\"Fit-5.1.1\">Fit</a></span></li><li><span><a href=\"#Fit-with-sklearn\" data-toc-modified-id=\"Fit-with-sklearn-5.1.2\">Fit with sklearn</a></span></li></ul></li><li><span><a href=\"#Test-against-sklearn:-fitting\" data-toc-modified-id=\"Test-against-sklearn:-fitting-5.2\">Test against sklearn: fitting</a></span></li><li><span><a href=\"#Test-against-sklearn:-predictions\" data-toc-modified-id=\"Test-against-sklearn:-predictions-5.3\">Test against sklearn: predictions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:21.621516Z",
     "start_time": "2018-07-30T11:10:21.500686Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# Logistic function\n",
    "The logisitc model is definied as follows.\n",
    "$$\n",
    "p(X) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}\\tag{4.6, logreg_prob}\\\\\n",
    "$$\n",
    "Simplify for substitution into (i) below:\n",
    "$$\n",
    "p(x_i) = \\frac{\\exp(\\beta x_i)}{1+\\exp(\\beta x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:22.332345Z",
     "start_time": "2018-07-30T11:10:22.328815Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "def logistic(x: float) -> float:\n",
    "    '''Take linear, return logistic.'''\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "def logreg_prob(x, coeff, intercept):\n",
    "    '''Return predicted probability in a one-versus-rest setting.'''\n",
    "    return logistic(np.dot(x, coeff.T).squeeze() + intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood function\n",
    "The likelihood (plausibility) of a particular logistic model $y=p(x)$ with the parameter set $\\beta$:\n",
    "$$\n",
    "\\require{action}\n",
    "\\mathscr{l}(\\beta) = \\prod_{\\texttip{i:y_i=1}{for i such that y_i = 1}}p(x_i)\\prod_{\\texttip{i':y_{i'}=0}{for i' such that y_i' = 0}}(1-p(x_{i'}))\\tag{4.5}\\\\\n",
    "$$\n",
    "Simplify expression as a Bernoulli distribution:\n",
    "$$\n",
    "\\mathscr{l}(\\beta) = \\prod_{i=0}^n[p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i}\\\\\n",
    "$$\n",
    "As $\\log(x)$ is monotonically increasing, maximising $\\mathscr{l}(\\beta)$ is equivalent to maximising $\\log\\mathscr{l}(\\beta)$, which may be simplified:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log\\mathscr{l}(\\beta) & = \\log(\\prod_{i=0}^n[p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i})\\\\\n",
    "& = \\sum_{i=0}^n\\log([p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i})\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i\\log~p(x_i)+(1-y_i)\\log[1-p(x_i)]\\right)\\tag{i}\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i\\log\\frac{\\exp(\\beta x_i)}{1+\\exp(\\beta x_i)}+(1-y_i)\\log\\left(1-\\frac{\\exp(\\beta x_i)}{1+\\exp(\\beta x_i)}\\right)\\right)\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i\\log\\frac{\\exp(\\beta x_i)}{1+\\exp(\\beta x_i)}+(1-y_i)\\log\\frac{1}{1+\\exp(\\beta x_i)}\\right)\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i[\\beta x_i-\\log(1 + \\exp(\\beta x_i)]+(1-y_i)[0-\\log(1 + \\exp(\\beta x_i)]\\right)\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i\\beta x_i-y_i\\log(1 + \\exp(\\beta x_i)-\\log[1 + \\exp(\\beta x_i)]+y_i\\log[1 + \\exp(\\beta x_i)]\\right)\\\\\n",
    "& = \\sum_{i=0}^n[y_i\\beta x_i-\\log(1 + \\exp(\\beta x_i)]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In order to find the maxima of $\\log\\mathscr{l}(\\beta)$ using gradient ascend, determine the differential (gradient) function, which may be simplified:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\beta}\\log\\mathscr{l}(\\beta) & = \\frac{d}{d\\beta}\\sum_{i=0}^n[y_i\\beta x_i-\\log(1 + \\exp(\\beta x_i)]\\\\\n",
    "& = \\sum_{i=0}^n[y_i x_i-\\frac{\\frac{d}{d\\beta}[1+exp(\\beta x_i)]}{1 + \\exp(\\beta x_i)}]\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i x_i-\\frac{0+exp(\\beta x_i)x_i}{1 + \\exp(\\beta x_i)}\\right)\\\\\n",
    "& = \\sum_{i=0}^n\\left(y_i -\\frac{exp(\\beta x_i)}{1 + \\exp(\\beta x_i)}\\right) x_i\\\\\n",
    "& = \\sum_{i=0}^n[y_i - p(x_i)] x_i\\tag{log_likelihood_gradient}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Further reading: [Why the gradient is the direction of steepest ascent](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/why-the-gradient-is-the-direction-of-steepest-ascent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:22.937414Z",
     "start_time": "2018-07-30T11:10:22.932906Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "def log_likelihood_gradient(x, y, beta):\n",
    "    '''Gradient for fitting using descend.'''\n",
    "    \n",
    "    if x.shape[1] - len(beta) == 1:\n",
    "        x = np.hstack((np.ones((len(x), 1)), x))\n",
    "        \n",
    "    return np.dot(y - logreg_prob(x[:,1:], beta[1:], beta[0]), x)\n",
    "\n",
    "def gradient_ascent(f, x_init, learning_rate, iterations):\n",
    "    '''\n",
    "    f : Gradient of the function to ascend\n",
    "    x_init : seeding input to start the ascent\n",
    "    '''\n",
    "    x = x_init\n",
    "    for i in range(iterations):\n",
    "        x += f(x) * learning_rate\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting: training predictors + training response --> model paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:23.468970Z",
     "start_time": "2018-07-30T11:10:23.463405Z"
    }
   },
   "outputs": [],
   "source": [
    "def logreg_fit(X, y:int, learning_rate=0.0001, iterations=3000):\n",
    "    '''\n",
    "    Fit logistic regression using maximum likelihood, recursively\n",
    "        over each target class (one-versus-rest).\n",
    "    If classification is binary, assume larger class as target.\n",
    "\n",
    "    Return coefficients, intercepts.\n",
    "    '''\n",
    "    beta_stack = np.empty((0, X.shape[1] + 1))\n",
    "    \n",
    "    targets = np.unique(y)\n",
    "    if len(targets) == 2:\n",
    "        targets = targets[1:2]\n",
    "        \n",
    "    for k in targets:\n",
    "        X_full = np.hstack((np.ones((len(X), 1)), X))  # add constant term\n",
    "        beta_init = np.zeros(X_full.shape[1])\n",
    "        gradient_function = lambda b: log_likelihood_gradient(X_full, y==k, b)\n",
    "        beta = gradient_ascent(gradient_function, beta_init, learning_rate, iterations)\n",
    "        \n",
    "        beta_stack = np.append(beta_stack, beta.reshape(1,-1), axis=0)\n",
    "        \n",
    "    return beta_stack[:,1:], beta_stack[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting: test predictors -- [model(parameters)] --> test response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:23.981919Z",
     "start_time": "2018-07-30T11:10:23.978424Z"
    }
   },
   "outputs": [],
   "source": [
    "def logreg_predict(x, coeff, intercept):\n",
    "    '''\n",
    "    Predict response class based on predicted probability of a\n",
    "        logistic regression fit.\n",
    "    '''\n",
    "    \n",
    "    if len(coeff) == 1:\n",
    "        return (logreg_prob(x, coeff, intercept) > 0.5).astype(int)\n",
    "    else:\n",
    "        return np.argmax(logreg_prob(x, coeff, intercept), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:25.328023Z",
     "start_time": "2018-07-30T11:10:24.922307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_observations (n): 150\n",
      "n_predictors (p): 3\n",
      "n_class (k): 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data[:,:-1]\n",
    "y = data.target\n",
    "# y = (data.target > 0).astype(int)  # uncomment this for binary classification\n",
    "print(\"n_observations (n): {}\".format(X.shape[0]))\n",
    "print(\"n_predictors (p): {}\".format(X.shape[1]))\n",
    "print(\"n_class (k): {}\".format(len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:26.279203Z",
     "start_time": "2018-07-30T11:10:25.817565Z"
    }
   },
   "outputs": [],
   "source": [
    "coeff, intercept = logreg_fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:26.355036Z",
     "start_time": "2018-07-30T11:10:26.281406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test against sklearn: fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:26.786875Z",
     "start_time": "2018-07-30T11:10:26.782580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn model parameters: [[ 0.3763905   1.54657021 -2.56915316]\n",
      " [ 0.60278842 -1.7292152  -0.07309094]\n",
      " [-1.82898401 -1.25199132  3.36769776]] [ 0.28215851  1.18099044 -1.37108097]\n",
      "my model parameters: [[ 0.3988074   1.57142293 -2.64498051]\n",
      " [ 0.38331814 -1.23543256  0.08002893]\n",
      " [-1.45550443 -1.25837044  2.77768663]] [ 0.29570172  0.38751133 -0.79073748]\n"
     ]
    }
   ],
   "source": [
    "print('sklearn model parameters:', clf.coef_, clf.intercept_)\n",
    "print('my model parameters:', coeff, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test against sklearn: predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T11:10:27.307192Z",
     "start_time": "2018-07-30T11:10:27.303140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions match: True\n"
     ]
    }
   ],
   "source": [
    "my_predict = logreg_predict(X, coeff, intercept)\n",
    "sklearn_predict = clf.predict(X)\n",
    "\n",
    "print('Predictions match:', (my_predict == sklearn_predict).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
